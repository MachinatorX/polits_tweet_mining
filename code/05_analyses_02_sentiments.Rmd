---
title: "06_analyses_02_sentiments"
author: "Sebastian Sauer"
date: "6 8 2017"
output: html_document
---


```{r setup, include=FALSE}

knitr::opts_knit$set(root.dir = normalizePath('../'))


knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      fig.align = "center",
                      cache = TRUE)
```




# Setup

First load some libraries.

```{r}
library(tidyverse)
library(readr)
library(lubridate)
library(magrittr)
library(tidytext)
library(stringr)
library(viridis)
library(wordcloud)
library(knitr)
library(viridis)
```


And the data.
```{r load-data}
load("../data_polit_twitter/tweets_df.Rdata")
load("../data_polit_twitter/tweet_tokens.Rdata")
load("../data_polit_twitter/polits_df.Rdata")

load("data/party_pal.Rdata")
sentiment_df <- read_csv("data/sentiment_df.csv")

```


# Prepare sentiment dictionaries
And the sentiment dictionaries, taken from this source:

R. Remus, U. Quasthoff & G. Heyer: SentiWS - a Publicly Available German-language Resource for Sentiment Analysis.
In: *Proceedings of the 7th International Language Ressources and Evaluation (LREC'10)*, pp. 1168-1171, 2010


**RUN ONLY once**, to prepare the dictionary.

```{r parse-sentiment-dics, echo = FALSE, include = FALSE}
# data source: http://wortschatz.uni-leipzig.de/en/download/

neg_df <- read_tsv("~/Documents/research/data/SentiWS_v1.8c_Negative.txt", 
                   col_names = FALSE)

names(neg_df) <- c("Wort_POS", "Wert", "Inflektionen")


neg_df %>% 
  tidyr::separate(col = Wort_POS, 
                  into = c("token", "POS")) -> neg_df


pos_df <- read_tsv("~/Documents/research/data/SentiWS_v1.8c_Positive.txt", col_names = FALSE)
names(pos_df) <- c("Wort_POS", "Wert", "Inflektionen")

pos_df %>% 
  tidyr::separate(col = Wort_POS, 
                  into = c("token", "POS")) -> pos_df


bind_rows("neg" = neg_df, "pos" = pos_df, .id = "neg_pos") -> sentiment_df


sentiment_df %>% 
  mutate_if(is.character, tolower) %>% 
  as_tibble -> sentiment_df

readr::write_csv(sentiment_df, "data/sentiment_df.csv")

rm(pos_df)
rm(neg_df)
```


# Simple sentiment analysis


## Check sentiment dictionnary

```{r}
names(sentiment_df)
```


## Perform sentiment analysis, simple

```{r}


sentiment_neg <- match(tweet_tokens$token, 
                       filter(sentiment_df, neg_pos == "neg")$token)
neg_score <- sum(!is.na(sentiment_neg))

# alternative way:

sentiment_neg <- tweet_tokens$token %in% 
  {sentiment_df %>% filter(neg_pos == "neg") %>% pull(token)}

sum(sentiment_neg)

sentiment_pos <- match(tweet_tokens$token, 
                       filter(sentiment_df, neg_pos == "pos")$token)
pos_score <- sum(!is.na(sentiment_pos))

sentiment_pos <- tweet_tokens$token %in% 
  {sentiment_df %>% filter(neg_pos == "pos") %>% pull(token)}
sum(sentiment_pos)

round(pos_score/neg_score, 3)
```

**Attention** It seems strange that this ratio is nearly equal to 2. Double check.


# Simple sentiment analysis per party


```{r}

tweet_tokens %>% 
  mutate(emo = case_when(
    token %in% sentiment_df$token[sentiment_df$neg_pos == "neg"] ~ "neg",
    token %in% sentiment_df$token[sentiment_df$neg_pos == "pos"] ~ "pos",
    TRUE ~ "none")
  ) %>% 
  count(party, emo) %>% 
  ungroup %>% 
  filter(party != "trump") %>% 
  group_by(party) %>% 
  mutate(prop = n/sum(n)) %>% 
  ungroup %>% 
  filter(emo != "none", !(party %in% c(NA, "fraktionslos"))) -> party_emo

kable(party_emo)


party_emo %>% 
  group_by(party) %>% 
  summarise(emo_sum = sum(prop),
            emo_diff = diff(prop)) -> party_emo_collapsed


kable(party_emo_collapsed)

```

Trump not included as the dictionary is for German language.

## Plot

```{r}
party_emo %>% 
  ggplot() +
  aes(x = party, y = prop) +
  geom_line(aes(group = emo), color = "grey80") +
  geom_point(aes(color = party, shape = emo)) +
  coord_flip() +
  labs(x = "Partei",
       y = "Anteil emotionaler Wörter",
       caption = "",
       shape = "Emotionsreichtung",
       color = "Partei") +
  scale_color_manual(values = party_pal) -> p_emo_word_count
    
p_emo_word_count  

ggsave(p_emo_word_count, file = "img/p_emo_word_count.pdf")


party_emo_collapsed %>% 
  rename(`Differenz pos. zu neg. Wörter` = emo_diff,
         `Summe pos. und neg. Wörter` = emo_sum,) %>% 
  gather(key = Art, value = Anteil, -party) %>% 
  ggplot() +
  aes(x = party, y = Anteil) +
  geom_line(aes(group = Art), color = "grey40") +
  geom_point(aes(color = party, shape = Art)) +
  coord_flip() +
  labs(title = "Anteil emotionaler Wörter",
       x = "Partei",
       y = "Anteil emotionaler Wörter") +
  scale_color_brewer(palette = "Set1",
                     name = "Partei") +
  theme(legend.position = "bottom") -> p_emo_words_collapsed

p_emo_words_collapsed

ggsave(p_emo_words_collapsed, file = "img/p_emo_words_collapsed.pdf")

```





## Emo ratios per party

```{r}
party_emo %>% 
  group_by(party) %>% 
  summarise(emo_pos_ratio = n[2]/(n[1])) %>% 
  arrange(-emo_pos_ratio) %>% 
  mutate(party = factor(party, party)) %>% 
  ggplot +
  aes(x = party, y = emo_pos_ratio) +
  geom_line(group = 1, color = "grey60") +
  geom_point(aes(color = party)) +
  labs(y = "Verhältnis von pos. zu neg. Wörtern",
       x = "Partei",
       color = "Partei") +
  coord_flip() +
  scale_color_manual(values = party_pal) -> p_emo_words_ratio

p_emo_words_ratio

ggsave(p_emo_words_ratio, file = "img/p_emo_words_ratio.pdf")
  
```

## Rm temp dfs

```{r}

rm(party_emo)
rm(party_emo_collapsed)
```



# Number of emo words per person (account/screenName)

## German

```{r comp-nr-of-emo-words-per-person}
names(tweet_tokens)

tweet_tokens %>% 
  filter(party != "trump") %>% 
  mutate(emo_type = case_when(
    token %in% sentiment_df$token[sentiment_df$neg_pos == "neg"] ~ "neg",
    token %in% sentiment_df$token[sentiment_df$neg_pos == "pos"] ~ "pos",
    TRUE ~ "none")
  ) %>% 
  filter(emo_type != "none") %>% 
  group_by(screenName, emo_type) %>% 
  summarise(emo_words_n = n()) %>% 
  spread(key = emo_type, value = emo_words_n) %>% 
  rename(neg_words_n = neg, pos_words_n = pos) %>% 
  mutate(emo_words_n = neg_words_n + pos_words_n,
         neg_words_ratio = neg_words_n / pos_words_n) %>% 
  ungroup -> screenName_emo_words



glimpse(screenName_emo_words)

```

## Join with main df and delete temp df

```{r}
polits_df %>% 
  left_join(screenName_emo_words) -> polits_df

rm(screenName_emo_words)
```


## Check if Trump emo words are still empty

```{r}
polits_df %>% 
  filter(party == "trump") %>% 
  glimpse
```




## English (Trump)

Get sentiments and join

```{r get-sentiments-and-join}

senti <- get_sentiments("bing") %>% 
  rename(token = word)


tweet_tokens %>% 
  filter(party == "trump") %>% 
  dplyr::filter(str_detect(token, "[a-z]")) %>% 
  inner_join(senti) %>% 
  group_by(sentiment) %>% 
  summarise(n = n()) %>% 
  spread(sentiment, n) -> trump_emo_words

trump_emo_words

save(trump_emo_words, 
     file = "../data_polit_twitter/trump_emo_words.Rdata")


trump_emo_words %>% names
screenName_emo_words %>% names
```




```{r}
trump_emo_words %>% 
  mutate(screenName = "realDonaldTrump") %>% 
  rename(neg_words_n = negative,
         pos_words_n = positive) %>% 
  mutate(emo_words_n = neg_words_n + pos_words_n,
         neg_words_ratio = neg_words_n / pos_words_n) -> trump_emo_words

trump_emo_words
names(polits_df)
```




Join Trump to rest.

```{r}

polits_df$neg_words_n[polits_df$screenName == "realDonaldTrump"] <- trump_emo_words$neg_words_n

polits_df$pos_words_n[polits_df$screenName == "realDonaldTrump"] <- trump_emo_words$pos_words_n

polits_df$emo_words_n[polits_df$screenName == "realDonaldTrump"] <- trump_emo_words$emo_words_n

polits_df$neg_words_ratio[polits_df$screenName == "realDonaldTrump"] <- trump_emo_words$neg_words_ratio
```


Double check if Trump is not included twice.

```{r}
polits_df %>% 
  select(screenName) %>% 
  n_distinct
```



```{r fig-screenName-emo-words}
polits_df %>% 
  ungroup %>% 
  #filter(!is.na(emo_words_n)) %>% 
  #na.omit %>% 
  arrange(-neg_words_ratio) %>% 
  slice(1:25) %>% 
  ggplot +
  aes(x = reorder(screenName, neg_words_ratio), y = neg_words_ratio) +
  geom_col(aes(fill = party)) +
  coord_flip() +
  labs(x = "Politiker",
       y = "Verhältnis von negativen zu positiven Wörtern",
       fill = "Partei",
       caption = "dargestellt sind die 25 'negativsten' Politiker") +
  theme(legend.position = "bottom") +
  scale_fill_manual(values = party_pal) -> p_neg_words_ratio

p_neg_words_ratio

ggsave(p_neg_words_ratio, file = 
"img/p_neg_words_ratio.pdf")

```


# Weighted Sentiment Analysis (z-scores)



## Compute sentiments per token

But not Trump, as the current dictionary is in German.

```{r join-sentiment-df}

names(sentiment_df)
names(tweet_tokens)
#sentiment_df %>% 
#  rename(token = Wort) -> sentiment_df


tweet_tokens %>% 
  filter(party %in% c(NA, "fraktionslos")) %>% 
  select(screenName, party) %>% 
  distinct

tweet_tokens %>% 
  filter(!(party %in% c(NA, "fraktionslos"))) %>% 
  inner_join(sentiment_df, by = "token") %>% 
  filter(party != "trump") -> tweet_tokens_sentis

```

## Some checks

```{r}
tweet_tokens_sentis %>% 
  summarise(na_rows = sum(!complete.cases(.)),
            na_rows_prop = round(na_rows / n(), 2),
            POS_unique = n_distinct(POS))

```

## compute sentiments scores per person


Now compute the emo scores

```{r comp-emo-scores-per-person}
tweet_tokens_sentis %>% 
  group_by(screenName) %>% 
  summarise(emo_score = (sum(Wert, na.rm = T) / n()) * 100,
            emo_abs_score = (sum(abs(Wert), na.rm = T) / n()) * 100) %>% 
  ungroup -> screenName_emo_score

glimpse(screenName_emo_score)

```


Join to main df and delete temp df `screenName_emo_score`

```{r}

polits_df %>% 
  left_join(screenName_emo_score) -> polits_df

rm(screenName_emo_score)
polits_df %>% ungroup -> polits_df
```


Group by party

```{r}
tweet_tokens_sentis %>% 
  group_by(party) %>% 
  summarise(emo_score = (sum(Wert, na.rm = T) / n()) * 100,
            emo_abs_score = (sum(abs(Wert), na.rm = T) / n()) * 100) %>% 
  ungroup -> party_emo_weighted

kable(party_emo_weighted)

party_emo_weighted %>% 
  ggplot +
  aes(x = reorder(party, emo_abs_score), y = emo_abs_score) +
  geom_col(aes(fill = party)) +
  coord_flip() +
  labs(x = "Partei",
       y = "Emotionswert") +
  scale_fill_manual(values = party_pal) -> p_party_emo_weights
  

ggsave(p_party_emo_weights, file = "img/p_party_emo_weights.pdf")

```


```{r p-senti}
polits_df %>% 
  filter(!party %in% c("fraktionslos", NA, "trump")) %>% 
  group_by(party) %>% 
  summarise(emo_score = median(emo_score, na.rm = T),
            emo_abs_score = median(emo_abs_score, na.rm = T)) %>% 
  ungroup %>% 
  select(party, emo_score, emo_abs_score) %>% 
  mutate(emo_score = scale(emo_score),
         emo_abs_score = scale(emo_abs_score)) %>% 
  gather(key = `Emo-Kennzahl`, `Relative Stärke`, -party) %>%
  mutate(`Emo-Kennzahl` = recode(`Emo-Kennzahl`,
                                 emo_abs_score = "Emotionalität",
                                 emo_score = "Positivität")) %>% 
  ggplot() +
  aes(x = party, y = `Relative Stärke`, color = `Emo-Kennzahl`) +
  geom_line(aes(group = `Emo-Kennzahl`), color = "grey40") +
  geom_point(aes(color = party, shape = `Emo-Kennzahl`), size = 5) +
  coord_flip() +
  scale_color_brewer(palette = "Set1") +
  labs(title = "Gewichtete Sentiment-Analyse \n(z-Werte) einiger Politiker-Tweets",
       x =  "Partei") +
  scale_color_manual(values = party_pal) -> p_senti

p_senti

ggsave(p_senti, file = "img/p_senti.pdf")
```



Print and plot the emo scores.

```{r kable-emo-scores-screenName}
polits_df %>% 
  select(screenName, emo_abs_score) %>% 
  arrange(-emo_abs_score) %>% 
  slice(1:10) %>% 
  kable
```

```{r plot-emo-scores-screenName}
polits_df %>% 
  select(screenName, emo_score, emo_abs_score, party) %>% 
  na.omit %>% 
  arrange(-emo_abs_score) %>% 
  filter(row_number() %in% 1:10 | row_number() %in% (n()-9):n()) %>% 
  #  slice(1:25) %>% 
  ggplot +
  aes(x = reorder(screenName, -emo_score), y = emo_score) +
  geom_line(group = 1) +
  geom_point(aes(color = party)) +
  coord_flip() +
  scale_color_manual(values = party_pal) +
  #scale_color_gradient(high = "red", low = "green") +
  labs(x = "Namen",
       y = "Emo-Score") -> p_emo_scores_screenNames

p_emo_scores_screenNames

ggsave(p_emo_scores_screenNames, file = "img/p_emo_scores_screenNames.pdf")
```




# What types of words are used and how often (Part of Speech (POS) tags)

## Check POS tags German

```{r}
polits_df %>% names
```


```{r}
tweet_tokens_sentis %>% 
  group_by(party, screenName, POS) %>% 
  summarise(n = n()) %>% 
  ungroup -> screenName_POS_n

```

now spread

```{r}
screenName_POS_n %>% 
  spread(key = POS, value = n) %>% 
  select(-party, screenName,
         adjx_n = adjx,
         adv_n = adv,
         vvinf_n = vvinf) -> screenName_POS_n_spread

```


## Join word usage per person to main df

```{r}
polits_df %>%
  left_join(screenName_POS_n_spread) -> polits_df
```



## Add Trump POS tags (English)

```{r}
parts_of_speech %>% 
  rename(token = word) -> POS_en

POS_en %>% 
  count(pos)

tweet_tokens %>% 
  filter(party == "trump") %>% 
  left_join(POS_en) %>% 
 # filter(pos %in% c("Adjective", "Adverb", "Noun")) %>% 
  mutate(POS = dplyr::recode(pos, 
                             Adjective = "adjx",
                             Adverb = "adv",
                             Noun = "nn",
                             `Verb (intransitive)` = "vvinf",
                             `Verb (transititive)` = "vvinf",
                             `Verb (usu participle)` = "vvinf",
                             .default = "NA"),
         POS = na_if(POS, "NA")) -> trump_POS

trump_POS %>% 
  filter(is.na(POS))


trump_POS %>% 
  count(POS)
```

Now summarise and spread

```{r comp-trump-word-usage-summary}
trump_POS %>% 
  select(screenName, POS) %>% 
  count(POS) %>% 
  filter(!is.na(POS)) %>% 
  spread(key = POS, value = n) %>% 
  rename(adjx_n = "adjx",
         adv_n = "adv",
         vvinf_n = "vvinf") %>% 
  mutate(screenName = "realDonaldTrump") -> trump_POS_summary


trump_POS_summary
```

Add to main df

```{r}
polits_df %>% 
  select(-c(adjx_n, adv_n, nn, vvinf_n)) %>% 
  filter(party == "trump") %>% 
  left_join(trump_POS_summary, by = "screenName") %>% 
  bind_rows(polits_df %>% filter(party != "trump")) -> polits_df
```



## Some checks

```{r}
trump_POS %>% 
  summarise(na_rows = sum(!complete.cases(.)),
            na_rows_prop = round(na_rows / n(), 2),
            POS_unique = n_distinct(POS))

```





```{r count-trump-word-types}

trump_POS %>% 
  count(pos)
```



```{r}

tweet_tokens_sentis %>% nrow
trump_POS %>% nrow

tweet_tokens_sentis %>% 
  filter(party != "trump") %>% 
  bind_rows(trump_POS) -> tweet_tokens_sentis


```


## Marginal

```{r}
tweet_tokens_sentis %>% 
  count(POS)
```

## Per party

Which party uses how many of what word types?
    
```{r p-word-types}
tweet_tokens_sentis %>% 
  filter(!is.na(POS)) %>% 
  group_by(party, POS) %>% 
  summarise(n = n()) %>% 
  ggplot +
  aes(x = POS, y = n) +
  geom_col(aes(fill = party)) +
  scale_fill_manual(values = party_pal) +
  facet_wrap(~party, scales = "free") -> p_word_types_n

p_word_types_n

ggsave(file = "img/p_word_types_n.pdf")

```


Ratio of adj to adverbs.

```{r}
tweet_tokens_sentis %>% 
  group_by(party, POS) %>% 
  summarise(n = n()) %>% 
  filter(POS %in% c("adv", "adjx")) %>% 
  spread(POS, n) %>% 
  mutate(adj_adv = adjx / adv) %>% 
  ggplot +
  aes(x = reorder(party, -adj_adv), y = adj_adv) +
  geom_col(aes(fill = party)) +
  scale_fill_manual(values = party_pal) +
  labs(x = "Partei",
       y = "Verhältnis von Adjektiven zu Adverben")  +
  coord_flip() -> p_adj_adv

p_adj_adv

ggsave(p_adj_adv, file = "img/p_adj_adv.pdf")
```



```{r}
screenName_POS_n %>% 
  filter(POS %in% c("adv", "adjx")) %>% 
  spread(POS, n) %>% 
  mutate(adj_adv = adjx / adv) %>% 
  ggplot +
  aes(x = reorder(party, -adj_adv), y = adj_adv) +
  geom_boxplot(aes(color = party, fill = party)) +
  coord_flip() -> p_adj_adv_ratio_2

p_adj_adv_ratio_2

ggsave(p_adj_adv_ratio_2, file = "img/p_adj_adv_ratio_2.pdf")
```






```{r comp-extreme-freqs-of-word-usage}
screenName_POS_n %>% 
  filter(POS %in% c("adv", "adjx")) %>% 
  spread(POS, n) %>% 
  mutate(adj_adv = adjx / adv) %>% 
  filter(!is.na(adj_adv)) %>% 
  arrange(-adj_adv) %>% 
  mutate(extreme = case_when(
    row_number() %in% 1:10 ~ "first",
    row_number() %in% c((n()-10):n()) ~ "last",
    TRUE ~ "in_between"
  )) %>% 
  filter(extreme %in% c("first", "last")) -> screenName_POS_n_extreme

```

Some checks

```{r}
screenName_POS_n_extreme %>% 
  filter(party %in% c("spd", "trump"))


screenName_POS_n %>% 
  filter(party %in% c("trump"))
```

```{r plot-extreme-usage-of-word-types}
screenName_POS_n_extreme %>% 
  ggplot +
  aes(x = reorder(screenName, -adj_adv), y = adj_adv) +
  geom_col(aes(color = party, fill = party)) +
  coord_flip() +
  scale_fill_manual(values = party_pal) +
  scale_color_manual(values = party_pal) +
  facet_wrap(~extreme, scales = "free") +
  labs(title = "Adjektiv-Adverb-Quote",
       y = "Verhältnis von Adjektiven zu Adverben",
       x = "Namen") -> p_top_adj_adv_quote
  
p_top_adj_adv_quote

ggsave(p_top_adj_adv_quote,
       file = "img/p_top_adj_adv_quote.pdf")

```



# Check

To double-check, let's compute the emo scores of two extreme cases again, to see whether the results are the same:

```{r}
tweet_tokens %>% 
  filter(party %in% c("afd", "csu")) %>% 
  left_join(sentiment_df, by = "token") %>% 
  group_by(party) %>% 
  summarise(emo_dif = sum(Wert, na.rm = T) / n(),
            emo_sum = sum(abs(Wert), na.rm = T) / n()) %>% 
  kable
```

Seems to be ok.




# Save data

```{r}
save(tweet_tokens_sentis, 
     file = "../data_polit_twitter/tweet_tokens_sentis.Rdata")

save(screenName_POS_n,
     file = "../data_polit_twitter/screenName_POS_n.Rdata")

save(polits_df,
     file = "../data_polit_twitter/polits_df.Rdata")
```

