---
title: "Analysing word counts"
author: "Sebastian Sauer"
date: "3 8 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = normalizePath('../'))


knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      fig.align = "center",
                      cache = TRUE)
```



Let's perform some initial analyses of polit tweets.

# Setup


First load some libraries.

```{r}
library(tidyverse)
library(readr)
library(lubridate)
library(magrittr)
library(tidytext)
library(stringr)
library(viridis)
library(wordcloud)
library(SnowballC)
library(knitr)
```


And the data.
```{r load-data}
load("../data_polit_twitter/tweets_df.Rdata")
polits_df <- read_csv("../data_polit_twitter/polits_df.csv")
```



# Tweet period 

period from first to most recent tweet


```{r}
tweets_df %>% 
  group_by(screenName) %>% 
  summarise(first_tweet = min(created),
            recent_tweet = max(created)) %>% 
  mutate(tweet_period = interval(first_tweet, recent_tweet) / ddays(1)) -> polit_periods


tweets_df %>% 
  left_join(polit_periods, by = "screenName") -> dummy


```






## How many accounts, how many tweets each?

## Who tweetet most?

```{r}
tweets_df %>% 
  count(screenName, sort = TRUE) -> tweets_per_screenName


tweets_per_screenName %>% 
  head(10)

tweets_per_screenName %>% 
  top_n(20) %>% 
  ggplot +
  aes(x = reorder(screenName, n), y = n) +
  geom_col() +
  coord_flip() +
  geom_text(aes(x = screenName, y = n, label = n))



```

These numbers cannot be (readily) interpreted, because the data are censored.


## How many accounts (per party)

In total, how many accounts are in the dataset?

```{r}
tweets_df %>% 
  count(screenName) %>% nrow
```

```{r}
tweets_df %>% 
  group_by(party) %>% 
  summarise(accounts_n = n_distinct(screenName)) %>% 
  ungroup %>% 
  ggplot() +
  aes(x = reorder(party, -accounts_n), y = accounts_n) + 
  geom_col()
```



## How many tweets per party

```{r}
tweets_df %>% 
  count(party) %>% 
  ggplot +
  aes(x = reorder(party, -n), y = n) +
  geom_col()
```


## Tweets per time

```{r fig-tweets-per-time, cache = TRUE}

tweets_df %>% 
  filter(!party %in% c(NA, "fraktionslos")) %>% 
  #sample_n(1000) %>% 
  ggplot +
  aes(x = created, y = party) +
  geom_jitter(aes(color = party)) -> p_tweets_per_time

p_tweets_per_time

tweets_df %>% 
  filter(!party %in% c(NA, "fraktionslos")) %>% 
  #sample_n(1000) %>% 
  ggplot +
  aes(y = created, x = party) +
  geom_boxplot(aes(fill = party, color = party)) +
  scale_fill_brewer(palette = "Set1") +
  scale_color_brewer(palette = "Set1") +
  coord_flip()

```


#


## Check tweets of one account: "ArminLaschet"

```{r}
tweets_df %>% 
  filter(screenName == "ArminLaschet") -> tweets_ArminLaschet_long

tweets_ArminLaschet_long %>% 
  nrow  # ~13k

```

Most recent/oldest tweet:

```{r}
tweets_ArminLaschet_long %>% 
  filter(screenName == "ArminLaschet") %>% 
  summarise(min(created),  # 2016-04-10 12:01:21 - 2017-08-02 21:17:45
            max(created))

tweets_ArminLaschet_long %>% 
  filter(created %in% c(min(created), max(created))) %>% 
  select(text)


```



For comparison, we can check whether the the duplicates filter has worked:

```{r}
tweets_df %>% 
  filter(screenName == "ArminLaschet") %>% 
  mutate(is_duplicate = duplicated(id)) %>% 
  filter(!is_duplicate) %>% nrow  # ~3k


tweets_df %>% 
  filter(screenName == "ArminLaschet") %>% 
  mutate(is_duplicate = duplicated(id)) %>% 
  filter(!is_duplicate) %>% nrow  # ~3k

```


Most recent/oldest tweet:


```{r}
tweets_df %>% 
  filter(screenName == "ArminLaschet") %>% 
  summarise(min(created),
            max(created))


```


# Oldest tweet in dataset

```{r}
tweets_df %>% pull(created) %>% min
```



# Get oldest/newest tweet

```{r}
tweets_df %>% 
  group_by(screenName) %>% 
  filter(id == min(id)) %>% 
  ungroup -> tweets_oldest  # weird error occurred

tweets_oldest %>% as.data.frame -> tweets_oldest


tweets_df %>% 
  dplyr::select(screenName, created) %>% 
  arrange(created) %>% 
  head(10) %>% 
  kable


tweets_df %>% 
  dplyr::select(screenName, created) %>% 
  arrange(desc(created)) %>% 
  head(10) %>% 
  kable



#save(tweets_oldest, file = paste0("data_polit_twitter/tweets_oldest_",lubridate::now()))
```


# Tweets per party


```{r}
tweets_df %>% 
  dplyr::count(party) %>% 
  ggplot +
  aes(x = reorder(party, n), y = n) +
  geom_col() +
  coord_flip()
  
```


# Tweets per person

```{r}
tweets_df %>% 
  group_by(party, screenName) %>% 
  summarise(n = n()) %>% 
  arrange(-n)
  
```


# daily tweets per person

```{r}
polits_df %>% 
  top_n(20) %>% 
  ggplot +
  aes(x = reorder(screenName, daily_tweets_n), y = daily_tweets_n) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n))
  
```

For instance, `c_lindner`:

First tweet in dataset:
```{r}
polits_df %>% 
  filter(screenName == "c_lindner") %>% 
  dplyr::select(n, first_tweet)
```


## Closer look

More than 3000 tweets in roughly a year? That's quite a lot; maybe something's not quite right under the woodshed in these data :(

Let's have a closer look.


```{r}
tweets_df %>% 
  filter(screenName == "c_lindner") -> lindner
```



How many days exactly do we have for this screenName?

```{r}
lindner %>% 
  summarise(first = min(created),
            last = max(created)) %>% 
  mutate(duration_weeks = (last - first) / dweeks(1),
         duration_days = (last - first) / ddays(1))
```

About 60 weeks, and 420 days.

Let's have a look at the distribution of his weekly tweets:

```{r}
lindner %>% 
  mutate(week_nr = (created - min(created)) / dweeks(1),
         day_nr = (created - min(created)) / ddays(1)) %>% 
  mutate(week_nr = round(week_nr),
         day_nr = round(day_nr)) -> lindner
  
lindner %>% 
  count(week_nr) %>% 
  ggplot() +
  aes(x = week_nr, y = n) +
  geom_col()

```


Let's compute som stats.

Mean:

```{r}
(lindner_daily_tweets_M <- nrow(lindner) / 420)

lindner %>% 
  count(day_nr) %>% 
  summarise(lindner_daily_tweets_m = mean(n),
            lindner_daily_tweets_sd = sd(n),
            lindner_daily_tweets_md = median(n),
            lindner_daily_tweets_iqr = IQR(n)) -> lindner_stats

```



And the distribution of the daily tweets:


```{r}



lindner %>% 
  count(day_nr) %>% 
  ggplot() +
  aes(x = day_nr, y = n) +
  geom_col() +
  geom_hline(yintercept = lindner_stats %>% 
               slice(1) %>% 
               pull(lindner_daily_tweets_m), 
             color = "blue") +
  geom_text(label = "M", aes(y= lindner_stats %>% 
               slice(1) %>% 
               pull(lindner_daily_tweets_m), x = 0), color = "blue") +
  geom_hline(yintercept = lindner_stats %>% 
               slice(1) %>% 
               pull(lindner_daily_tweets_md), 
             color = "red") +
  geom_text(label = "Md", aes(y= lindner_stats %>% 
               slice(1) %>% 
               pull(lindner_daily_tweets_md), x = 0), color = "red")

```

## Duplicates for Lindner?

Looks all quite reasonable. 

Duplicates?

```{r}
lindner %>% 
  mutate(is_duplicate = duplicated(id)) %>% 
  filter(is_duplicate) %>% nrow
```

Nup.


## Look at some peak days


```{r}
lindner %>% 
 count(day_nr) %>% 
  mutate(my_date = ymd("2016-06-09") + day_nr) %>% 
  arrange(-n)
```

Ok, let's check Twitter how much he tweeted on 2017-05-11.

...
It was Rally day in NRW...
...


Hm, I counted roughly 38. Dataframes says 50. Maybe some deletion?

Let's look at those 50.

```{r}
lindner %>% 
  filter(created > dmy("10-05-2017"), created < dmy("12-05-2017")) %>% 
  select(text, created, id) -> lindner_most_tweeted
```

## Tweets deleted?

Interesting. There are tweets missing in the timeline of `c_lindner`. For instance, tweets 4 to 6 in `lindner_most_tweeted` are appearing in the timeline on Twitter. The ids are:

```{r}
lindner_most_tweeted %>% 
  slice(4:6) %>% 
  select(id)
```

## Conclusion

OK, so it appears there's an explanation in the differing number of tweets: There are some tweets missing in the timeline on the Website, appearingly deleted by the owner of the account.



# Daily tweets per party

```{r}

polits_df %>% 
  group_by(party) %>%
  summarise(t_pd_m = mean(daily_tweets_n),
            n_polits = n()) %>% 
  arrange(-t_pd_m) %>% 
  ungroup %>% 
  mutate(party = factor(party, party)) %>% 
  ggplot +
  aes(x = party, y = t_pd_m) +
  geom_point(aes(size = n_polits)) +
  labs(title = "Mean daily number of tweets as per party",
       caption = "size reflects number of Twitter accounts") +
  scale_radius()
```



# Most frequent words per party


## Read stopwords, unnest tokens, compute frequencies
```{r read-stopwords}
data(stopwords_de, package = "lsa")
stopwords_de <- data_frame(token = stopwords_de)

stopwords_de_ses <- read_csv("data/stopwords_de_ses.csv")

stopwords_de_ses %>% 
  rename(token = stopword) -> stopwords_de_ses

```

Now unnest the tokens (1-grams):

```{r unnest-tokens}
tweets_df %>% 
  select(party, text, screenName) %>% 
  unnest_tokens(output = token, input = text) %>%  
  dplyr::filter(str_detect(token, "[a-z]")) -> tweets_token_including_stopwords

```

`unnest_tokens` performs `str_to_lower` too.


```{r antijoin-stopwords-to-tokens}
tweets_token_including_stopwords %>% 
  anti_join(stopwords_de_ses, by = "token") %>% 
  anti_join(stopwords_de, by = "token") -> tweet_tokens
```


This one needs some seconds:

```{r}
#load("../data_polit_twitter/tweet_tokens.Rdata")
tweet_tokens %>% 
  count(token, sort = TRUE) -> tweets_word_count

```

```{r}
tweets_word_count %>% 
  top_n(100) %>% 
  as.data.frame %>% 
  arrange(-n)
```

## Visualize summary frequencies (bars)


```{r}
tweets_word_count %>% 
  top_n(25) %>% 
  arrange(-n) %>% 
  mutate(token = factor(token, levels = rev(token))) %>% 
  ggplot +
  aes(x = token, y = n) +
  geom_col() +
  scale_fill_viridis() +
  coord_flip()  +
  labs("Welche Wörter wurden am häufigsten getweetet?",
       caption = "Stopwörter bereits entfernt.")
```


## wordcloud, summary frequencies

```{r}
wordcloud(words = tweets_word_count$token, 
          freq = tweets_word_count$n, 
          max.words = 100, 
          scale = c(2,.5), 
          colors=brewer.pal(6, "Dark2"))
```


## Most frequent terms per party


```{r}

load("../data_polit_twitter/tweet_tokens.Rdata")


tweet_tokens %>% 
  count(party, token, sort = TRUE) -> tweets_per_party_word_count

```


## Plot frequencies par party (bars)
```{r}

tweets_per_party_word_count %>% 
  filter(!(party %in% c("NA", "fraktionslos", NA))) %>% 
  sample_n(1000) %>% 
  group_by(party) %>% 
  arrange(-n) %>% 
  slice(1:10) %>% 
  ungroup %>% 
  mutate(token = factor(token)) %>% 
  ggplot() +
  aes(x = reorder(token, n), y = n) +
  geom_col() +
  facet_wrap(~party, scales = "free", drop = TRUE) +
  coord_flip() -> plot_tweets_per_party_word_count

plot_tweets_per_party_word_count

```


## Same, but with Wordstemming


```{r}
tweet_tokens %>% 
  mutate(token = wordStem(.$token, language = "german")) %>% 
  count(party, token, sort = TRUE) -> tweets_per_party_word_count_stemmed


names(tweets_per_party_word_count_stemmed)
save(tweets_per_party_word_count_stemmed,
     file = "../data_polit_twitter/tweets_per_party_word_count_stemmed.Rdata")
```




```{r}


tweets_per_party_word_count_stemmed %>% 
  filter(!(party %in% c("NA", "fraktionslos", NA))) %>% 
  sample_n(1000) %>% 
  group_by(party) %>% 
  arrange(-n) %>% 
  slice(1:10) %>% 
  ungroup %>% 
  mutate(token = factor(token)) %>% 
  ggplot() +
  aes(x = reorder(token, n), y = n) +
  geom_col() +
  facet_wrap(~party, scales = "free", drop = TRUE) +
  coord_flip()

```


## Stemm party variants (spdde --> spd)

Also, there are duplicates such as "spdbt" "spdde" and "spd"; those instances should be stemmed to "spd", and the like.

```{r}

stem_term <- function(df, col, stem_output){
 # this function looks for '\\w+stem_output\\w+' and replaces each element by `stem_output`
  
    # parameters: 
  # df: data, 
  # col: column to be stemmed, 
  # stem_output: desired output after stemming
  
  # output
  # vector of tokens, where hits are replaced
  
  
  col_quo <- enquo(col)
  col_name <- quo_name(col_quo)
  stem_quo <- enquo(stem_output)
  stem_name <- quo_name(stem_quo)
  
  
  # gsub/str_replace do not understands functions with more than 1 parameter
  pattern1 <- paste0(stem_output,"\\w+")
  pattern2 <- paste0("\\w+",stem_output)
  
    
    df %>% 
      mutate(col_stemmed = str_replace(string = !!col_quo,
                                       pattern = pattern1,
                                       replacement = 
                                       stem_output)) %>% 
      # mutate(col_stemmed = str_replace(string = !!col_quo,
      #                                  pattern = pattern2,
      #                                  replacement = 
      #                                  stem_output)) %>% 
    pull(col_stemmed) -> output
  
  
  return(output)
 
}
```

For comparison, execute this function non-programmatically, but interactively (for debugging reasons):

```{r}
party <- "afd"

my_pattern <- paste0(party, "\\w+")

tweets_per_party_word_count_stemmed %>% 
  mutate(output = str_replace(string = .$token,
                         pattern = my_pattern,
                         replacement = "afd")) %>% 
  filter(str_detect(token, "afd"))



```


Now see whether the function runs as expected:


```{r}
#debug(stem_party)
dummy <- stem_term(df = tweets_per_party_word_count_stemmed,
                     col = token,
                     stem_output = "afd")

dummy %>% 
  as_tibble %>% 
  filter(str_detect(dummy, "afd"))
```



Get different parties:

```{r}
parties <- polits_df %>% filter(!is.na(party)) %>% pull(party) %>% unique
parties
```



```{r}
tweets_per_party_word_count_stemmed2 <- tweets_per_party_word_count_stemmed


for(i in seq_along(parties)){
  tweets_per_party_word_count_stemmed2$token <- 
    stem_term(df = tweets_per_party_word_count_stemmed,
              col = token,
              stem_output = parties[i])  
  
}

```

Plot again with party-related terms stemmed:

```{r}
tweets_per_party_word_count_stemmed2 %>% 
  filter(!(party %in% c("NA", "fraktionslos", NA))) %>% 
  sample_n(1000) %>% 
  group_by(party) %>% 
  arrange(-n) %>% 
  slice(1:10) %>% 
  ungroup %>% 
  mutate(token = factor(token)) %>% 
  ggplot() +
  aes(x = reorder(token, n), y = n) +
  geom_col() +
  facet_wrap(~party, scales = "free", drop = TRUE) +
  coord_flip()

```



# Word lengths

```{r}
tweet_tokens %>% 
  mutate(word_length = str_length(token)) %>% 
  group_by(screenName) %>% 
  ggplot +
  aes(x = word_length) +
  geom_histogram() +
  xlim(0,30) +
  labs(title = "Mittlere Wortlänge in Tweets der Politiker")
 
```

```{r}
tweet_tokens %>% 
  mutate(word_length = str_length(token)) %>% 
  group_by(screenName) %>% 
  summarise(word_length_avg = mean(word_length, na.rm = T),
            word_length_md = median(word_length, na.rm = T)) -> screenName_word_lengths


```



# What's speaks the AfD about what the others are not interested in?

## Top 100 token per party
```{r}

parties_df <- data_frame(
  party = parties,
  party_ID = 1:length(parties)
)
tweets_per_party_word_count_stemmed %>% 
  mutate(party = str_to_lower(party)) %>% 
  group_by(party) %>% 
  arrange(-n) %>% 
  slice(1:100) %>% 
  left_join(parties_df, by = "party") %>% 
  filter(!is.na(party_ID)) -> top_100s

top_100s %>% 
  count(party_ID)
```


This one is still buggy:


```{r eval = FALSE}
mat <- matrix(0, nrow = 9, ncol = 9)

setdiff(top_100s$token[top_100s$party_ID == 8],
        top_100s$token[top_100s$party_ID == 9])


outer(1:nrow(mat), 1:ncol(mat), FUN = function(i,j) setdiff(top_100s$token[top_100s$party_ID == i],top_100s$token[top_100s$party_ID == j]))



```




# Save token data



```{r}

save(tweet_tokens, file = "../data_polit_twitter/tweet_tokens.Rdata")

save(tweets_token_including_stopwords, file = "../data_polit_twitter/tweets_token_including_stopwords.Rdata")

save(screenName_word_lengths,
     file = "../data_polit_twitter/screenName_word_lengths.Rdata")

```

