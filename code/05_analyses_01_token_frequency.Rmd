---
title: "Analysing word counts"
author: "Sebastian Sauer"
date: "3 8 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = normalizePath('../'))


knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      fig.align = "center",
                      cache = TRUE)
```



---

 Work in progress. DRAFT.

---



Let's perform some initial analyses of polit tweets.

# Setup


First load some libraries.

```{r}
library(tidyverse)
library(readr)
library(lubridate)
library(magrittr)
library(tidytext)
library(stringr)
library(viridis)
library(wordcloud)
library(SnowballC)
library(knitr)
```


## Load the data

And the data.
```{r load-data}
load("../data_polit_twitter/tweets_df.Rdata")
load("../data_polit_twitter/polits_df.Rdata")
load("data/party_pal.Rdata")

data(stopwords_de, package = "lsa")
stopwords_de_ses <- read_csv("data/stopwords_de_ses.csv")


```

## Min 2 tweets

Only consider screenNames with more than 1 tweet.

```{r}
polits_df %>% 
  filter(n == 1)  %>% 
  select(screenName, party) -> accounts_with_one_tweet
```

```{r}
polits_df %>% 
  filter(n > 1) -> polits_df
```


```{r}
tweets_df %>% 
  filter(screenName %in% accounts_with_one_tweet$screenName) %>% nrow
```


```{r}
tweets_df %>% 
  filter(!screenName %in% accounts_with_one_tweet$screenName) -> tweets_df
```


# Tweet period 

period from first to most recent tweet


```{r}
names(tweets_df)

tweets_df %>% 
  group_by(screenName) %>% 
  summarise(first_tweet = min(created),
            recent_tweet = max(created)) %>% 
  mutate(tweet_period = interval(first_tweet, recent_tweet) / ddays(1)) -> polit_periods

```


## How many accounts, how many tweets each?

## Who tweetet most?

```{r}
tweets_df %>% 
  count(screenName, sort = TRUE) -> tweets_per_screenName


tweets_per_screenName %>% 
  head(10)

tweets_per_screenName %>% 
  top_n(20) %>% 
  ggplot +
  aes(x = reorder(screenName, n), y = n) +
  geom_col() +
  coord_flip() +
  geom_text(aes(x = screenName, y = n, label = n)) +
  scale_fill_manual(values = party_pal)



```

These numbers cannot be (readily) interpreted, because the data are censored.


## How many accounts (per party)

In total, how many accounts are in the dataset?

```{r}
tweets_df %>% 
  count(screenName) %>% nrow
```

```{r}
tweets_df %>% 
  group_by(party) %>% 
  summarise(accounts_n = n_distinct(screenName)) %>% 
  ungroup %>% 
  filter(!is.na(party)) %>% 
  ggplot() +
  aes(x = reorder(party, accounts_n), y = accounts_n) + 
  geom_col(aes(fill = party)) +
  coord_flip() +
  labs(x = "Partei",
       y = "Anzahl Accounts") +
  scale_fill_manual(values = party_pal) -> p_accounts_per_party

p_accounts_per_party

ggsave(p_accounts_per_party, file = "img/p_accounts_per_party.pdf")
```

### Closer look to 'fraktionslos'

```{r}
tweets_df %>% 
  filter(party == "fraktionslos") %>% 
  group_by(screenName) %>% 
  summarise(n = n())
```


## How many tweets per party

```{r}
tweets_df %>% 
  count(party) %>% 
  ungroup %>% 
  filter(!is.na(party)) %>% 
  ggplot +
  aes(x = reorder(party, n), y = n) +
  geom_col(aes(fill = party)) +
  coord_flip() +
  labs(x = "Partei",
       y = "Anzahl Tweets") +
  scale_fill_manual(values = party_pal) -> p_tweets_per_party

p_tweets_per_party


ggsave(p_tweets_per_party, file = "img/p_tweets_per_party.pdf")

```


## Tweets per time

```{r fig-tweets-timeline1, cache = TRUE}

# needs some time to plot
tweets_df %>% 
  filter(!party %in% c(NA, "fraktionslos")) %>% 
  #sample_n(1000) %>% 
  ggplot +
  aes(x = created, y = party) +
  geom_jitter(aes(color = party)) +
  scale_fill_manual(values = party_pal) -> p_tweets_per_time

p_tweets_per_time

ggsave(p_tweets_per_time, filename = "img/p_tweets_per_time.png")
```


```{r}

tweets_df$party %>% str

tweets_df %>% 
  filter(!party %in% c(NA, "fraktionslos")) %>% 
  #sample_n(1000) %>% 
  ggplot +
  aes(y = created, x = party) +
  geom_boxplot(aes(fill = party, color = party)) +
  scale_fill_manual(values = party_pal) +
  scale_color_manual(values = party_pal) +
  coord_flip() -> fig_tweets_timeline1


fig_tweets_timeline1

ggsave(fig_tweets_timeline1, filename = "img/fig_tweets_timeline1.pdf")


```


```{r}
tweets_df %>% 
  filter(!party %in% c(NA, "fraktionslos")) %>% 
  ggplot +
  aes(x = party, y = created, fill = party, color = party) +
  geom_violin() +
  labs(x = "Partei",
       y = "Jahr",
       Fill = "Partei") +
  scale_fill_manual(values = party_pal) +
  scale_color_manual(values = party_pal) +
  coord_flip() -> p_tweets_timeline2

p_tweets_timeline2

ggsave(p_tweets_timeline2, file = "img/p_tweets_timeline2.pdf")
ggsave(p_tweets_timeline2, file = "img/p_tweets_timeline2.png")

```



## Check tweets of one account: "ArminLaschet"

```{r}
tweets_df %>% 
  filter(screenName == "ArminLaschet") -> tweets_ArminLaschet_long

tweets_ArminLaschet_long %>% 
  nrow  # ~13k

```

Most recent/oldest tweet:

```{r}
tweets_ArminLaschet_long %>% 
  filter(screenName == "ArminLaschet") %>% 
  summarise(min(created),  # 2016-04-10 12:01:21 - 2017-08-02 21:17:45
            max(created))

tweets_ArminLaschet_long %>% 
  filter(created %in% c(min(created), max(created))) %>% 
  select(text)


```



For comparison, we can check whether the the duplicates filter has worked:

```{r}
tweets_df %>% 
  filter(screenName == "ArminLaschet") %>% 
  mutate(is_duplicate = duplicated(id)) %>% 
  filter(!is_duplicate) %>% nrow  # ~3k


tweets_df %>% 
  filter(screenName == "ArminLaschet") %>% 
  mutate(is_duplicate = duplicated(id)) %>% 
  filter(!is_duplicate) %>% nrow  # ~3k

```


Most recent/oldest tweet:


```{r}
tweets_df %>% 
  filter(screenName == "ArminLaschet") %>% 
  summarise(min(created),
            max(created))


```


# Oldest tweet in dataset

```{r}
tweets_df %>% pull(created) %>% min
```



# Get oldest/newest tweet

```{r}
tweets_df %>% 
  group_by(screenName) %>% 
  filter(id == min(id)) %>% 
  ungroup -> tweets_oldest  # weird error occurred

tweets_oldest %>% as.data.frame -> tweets_oldest


tweets_df %>% 
  dplyr::select(screenName, created) %>% 
  arrange(created) %>% 
  head(10) %>% 
  kable


tweets_df %>% 
  dplyr::select(screenName, created) %>% 
  arrange(desc(created)) %>% 
  head(10) %>% 
  kable



#save(tweets_oldest, file = paste0("data_polit_twitter/tweets_oldest_",lubridate::now()))
```



# Tweets per person

```{r}
tweets_df %>% 
  group_by(party, screenName) %>% 
  summarise(n = n()) %>% 
  arrange(-n)
  
```



## Compute words per screenName

```{r}
tweets_df %>% 
  group_by(screenName) %>% 
  summarise(word_count = str_count(text) %>% sum) %>% 
  ungroup %>% 
  full_join(polits_df, by = "screenName") -> polits_df
  
```


## Checks

```{r}

if (any(is.na(polits_df$screenName))) cat("There are screenNames missing!")
if (any(is.na(polits_df$party))) cat("There are party names missing!")

polits_df %>% 
  select(screenName) %>% 
  n_distinct

```


# Daily tweets per party

## Mean
```{r}


polits_df %>% 
  group_by(party) %>% 
  filter(n > 1) %>% 
  summarise(tweets_m = mean(daily_tweets_n)) %>% 
  na.omit %>% 
  ggplot +
  aes(x = reorder(party, tweets_m), y = tweets_m) +
  geom_col(aes(fill = party)) +
  labs(x = "Partei",
       y = "Mittlere Anzahl (Durchschnitt) an Tweets pro Tag") +
  scale_fill_manual(values = party_pal) +
  coord_flip() -> p_tweets_day_party_mean

p_tweets_day_party_mean

ggsave(p_tweets_day_party, file = "img/p_tweets_day_party_mean.pdf")
```


## Median 
```{r}

polits_df %>% 
  group_by(party) %>% 
  summarise(tweets_md = median(daily_tweets_n, na.rm = T)) %>% 
  filter(!is.na(party)) %>% 
  ggplot() +
  aes(x = reorder(party, tweets_md), y = tweets_md) +
  geom_col(aes(fill = party)) +
  labs(x = "Partei",
       y = "Mediane Anzahl an Tweets pro Tag") +
  scale_fill_manual(values = party_pal) +
  coord_flip() -> p_tweets_day_party_md

p_tweets_day_party_md

ggsave(p_tweets_day_party_md,
       file = "img/p_tweets_day_party_md.pdf")

```


## Without Trump, without 'fraktionslos'


```{r}

polits_df %>% 
  filter(!party %in% c("trump", "fraktionslos")) %>% 
  group_by(party) %>% 
  summarise(tweets_md = median(daily_tweets_n, na.rm = T)) %>% 
  filter(!is.na(party)) %>% 
  ggplot() +
  aes(x = reorder(party, tweets_md), y = tweets_md) +
  geom_col(aes(fill = party)) +
  labs(x = "Partei",
       y = "Mediane Anzahl an Tweets pro Tag") +
  scale_fill_manual(values = party_pal) +
  coord_flip() -> p_tweets_day_party_md_no_trump

p_tweets_day_party_md_no_trump

ggsave(p_tweets_day_party_md_no_trump,
       file = "img/p_tweets_day_party_md_no_trump.pdf")
ggsave(p_tweets_day_party_md_no_trump,
       file = "img/p_tweets_day_party_md_no_trump.png")

```



# daily tweets per person

```{r}
polits_df %>% 
  top_n(20) %>% 
  ggplot +
  aes(x = reorder(screenName, daily_tweets_n), y = daily_tweets_n) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n)) +
  
  
```


## Closer look to `c_linder`


For instance, `c_lindner`:

First tweet in dataset:
```{r}
polits_df %>% 
  filter(screenName == "c_lindner") %>% 
  dplyr::select(n, first_tweet)
```

### Number of tweets per year

More than 3000 tweets in roughly a year? That's quite a lot; maybe something's not quite right under the woodshed in these data :(

Let's have a closer look.


```{r}
tweets_df %>% 
  filter(screenName == "c_lindner") -> lindner
```



How many days exactly do we have for this screenName?

```{r}
lindner %>% 
  summarise(first = min(created),
            last = max(created)) %>% 
  mutate(duration_weeks = (last - first) / dweeks(1),
         duration_days = (last - first) / ddays(1))
```

About 60 weeks, and 420 days.

Let's have a look at the distribution of his weekly tweets:

```{r}
lindner %>% 
  mutate(week_nr = (created - min(created)) / dweeks(1),
         day_nr = (created - min(created)) / ddays(1)) %>% 
  mutate(week_nr = round(week_nr),
         day_nr = round(day_nr)) -> lindner
  
lindner %>% 
  count(week_nr) %>% 
  ggplot() +
  aes(x = week_nr, y = n) +
  geom_col()

```


Let's compute som stats.

Mean:

```{r}
(lindner_daily_tweets_M <- nrow(lindner) / 420)

lindner %>% 
  count(day_nr) %>% 
  summarise(lindner_daily_tweets_m = mean(n),
            lindner_daily_tweets_sd = sd(n),
            lindner_daily_tweets_md = median(n),
            lindner_daily_tweets_iqr = IQR(n)) -> lindner_stats

```



And the distribution of the daily tweets:


```{r}



lindner %>% 
  count(day_nr) %>% 
  ggplot() +
  aes(x = day_nr, y = n) +
  geom_col() +
  geom_hline(yintercept = lindner_stats %>% 
               slice(1) %>% 
               pull(lindner_daily_tweets_m), 
             color = "blue") +
  geom_text(label = "M", aes(y= lindner_stats %>% 
               slice(1) %>% 
               pull(lindner_daily_tweets_m), x = 0), color = "blue") +
  geom_hline(yintercept = lindner_stats %>% 
               slice(1) %>% 
               pull(lindner_daily_tweets_md), 
             color = "red") +
  geom_text(label = "Md", aes(y= lindner_stats %>% 
               slice(1) %>% 
               pull(lindner_daily_tweets_md), x = 0), color = "red")

```

### Duplicates for Lindner?

Looks all quite reasonable. 

Duplicates?

```{r}
lindner %>% 
  mutate(is_duplicate = duplicated(id)) %>% 
  filter(is_duplicate) %>% nrow
```

Nup.


### Look at some peak days


```{r}
lindner %>% 
 count(day_nr) %>% 
  mutate(my_date = ymd("2016-06-09") + day_nr) %>% 
  arrange(-n)
```

Ok, let's check Twitter how much he tweeted on 2017-05-11.

...
It was Rally day in NRW...
...


Hm, I counted roughly 38. Dataframes says 50. Maybe some deletion?

Let's look at those 50.

```{r}
lindner %>% 
  filter(created > dmy("10-05-2017"), created < dmy("12-05-2017")) %>% 
  select(text, created, id) -> lindner_most_tweeted
```

### Tweets deleted?

Interesting. There are tweets missing in the timeline of `c_lindner`. For instance, tweets 4 to 6 in `lindner_most_tweeted` are appearing in the timeline on Twitter. The ids are:

```{r}
lindner_most_tweeted %>% 
  slice(4:6) %>% 
  select(id)
```

### Conclusion

OK, so it appears there's an explanation in the differing number of tweets: There are some tweets missing in the timeline on the Website, appearingly deleted by the owner of the account.


## Closer look: `EskenSaskia`


### Glimpse
```{r}
polits_df %>% 
  filter(screenName == "EskenSaskia") %>% 
  glimpse
```


### Peak days

```{r}
tweets_df %>% 
  filter(screenName == "EskenSaskia") %>% 
  mutate(day = date(created)) %>% 
  count(day, sort = TRUE)
```


### August

```{r}
tweets_df %>% 
  filter(screenName == "EskenSaskia") %>% 
  mutate(day = date(created)) %>% 
  filter(month(day) == 8) %>% 
  count(day, sort = TRUE)
```

It appears true: This person tweets a lot; more than Trump. I have checked on the Twitter site, and there is even ways more tweets than were delieverd via the API. She simply tweets a lot.



# Daily tweets per party

```{r}
polits_df %>% 
  group_by(party) %>%
  summarise(t_pd_md = median(daily_tweets_n),
            n_polits = n()) %>% 
  arrange(-t_pd_md) %>% 
  ungroup %>% 
  mutate(party = factor(party, party)) %>% 
  ggplot +
  aes(x = party, y = t_pd_md) +
  geom_point(aes(size = n_polits, color = party)) +
  labs(y = "Mediane Anzahl von Tweets",
       x = "Partei",
       caption = "Punktgröße entspricht der Anzahl der Accounts") +
  scale_radius() +
  scale_color_manual(values = party_pal)
```


# Most frequent words per party


## Read stopwords, unnest tokens, compute frequencies
```{r read-stopwords}
stopwords_de <- data_frame(token = stopwords_de)

stopwords_de_ses %>% 
  rename(token = stopword) -> stopwords_de_ses

```

Now unnest the tokens (1-grams):

```{r unnest-tokens}
tweets_df %>% 
  select(party, text, screenName) %>% 
  unnest_tokens(output = token, input = text) %>%  
  dplyr::filter(str_detect(token, "[a-z]")) -> tweets_token_including_stopwords

```

### Checks

Trump included?
```{r}
tweets_token_including_stopwords %>% 
  filter(party == "trump")
```



`unnest_tokens` performs `str_to_lower` too.


```{r antijoin-stopwords-to-tokens}
tweets_token_including_stopwords %>% 
  anti_join(stopwords_de_ses, by = "token") %>% 
  anti_join(stopwords_de, by = "token") -> tweet_tokens
```


Trump included?
```{r}
tweet_tokens %>% 
  filter(party == "trump")
```


This one needs some seconds:

```{r}
#load("../data_polit_twitter/tweet_tokens.Rdata")
tweet_tokens %>% 
  count(token, sort = TRUE) -> tweets_word_count


```

```{r}
tweets_word_count %>% 
  top_n(100) %>% 
  as.data.frame %>% 
  arrange(-n)
```

## Visualize summary frequencies (bars)


```{r}
tweets_word_count %>% 
  top_n(25) %>% 
  arrange(-n) %>% 
  mutate(token = factor(token, levels = rev(token))) %>% 
  ggplot +
  aes(x = token, y = n) +
  geom_col() +
  #scale_fill_manual(values = party_pal) +
  coord_flip()  +
  labs("Welche Wörter wurden am häufigsten getweetet?",
       caption = "Stopwörter bereits entfernt.") -> p_freq_words_total

p_freq_words_total

ggsave(p_freq_words_total, file = "img/p_freq_words_total.pdf")
```


## wordcloud, summary frequencies

```{r}
wordcloud(words = tweets_word_count$token, 
          freq = tweets_word_count$n, 
          max.words = 100, 
          scale = c(2,.5), 
          colors=brewer.pal(6, "Dark2"))
```


## Most frequent terms per party


```{r}

#load("../data_polit_twitter/tweet_tokens.Rdata")


tweet_tokens %>% 
  count(party, token, sort = TRUE) -> tweets_per_party_word_count

```


## Plot frequencies par party (bars)
```{r}

tweets_per_party_word_count %>% 
  filter(!(party %in% c("NA", "fraktionslos", NA))) %>% 
  sample_n(1000) %>% 
  group_by(party) %>% 
  arrange(-n) %>% 
  slice(1:10) %>% 
  ungroup %>% 
  mutate(token = factor(token)) %>% 
  ggplot() +
  aes(x = reorder(token, n), y = n) +
  geom_col() +
  facet_wrap(~party, scales = "free", drop = TRUE) +
  coord_flip() -> plot_tweets_per_party_word_count

plot_tweets_per_party_word_count

ggsave(plot_tweets_per_party_word_count,
       file = "img/plot_tweets_per_party_word_count.pdf")
```


## Same, but with Wordstemming


```{r}
tweet_tokens %>% 
  mutate(token = wordStem(.$token, language = "german")) %>% 
  count(party, token, sort = TRUE) %>% 
  ungroup -> tweets_per_party_word_count_stemmed


names(tweets_per_party_word_count_stemmed)
save(tweets_per_party_word_count_stemmed,
     file = "../data_polit_twitter/tweets_per_party_word_count_stemmed.Rdata")
```




```{r}


tweets_per_party_word_count_stemmed %>% 
  filter(!(party %in% c("NA", "fraktionslos", NA))) %>% 
  sample_n(1000) %>% 
  group_by(party) %>% 
  arrange(-n) %>% 
  slice(1:10) %>% 
  ungroup %>% 
  mutate(token = factor(token)) %>% 
  ggplot() +
  aes(x = reorder(token, n), y = n) +
  geom_col() +
  facet_wrap(~party, scales = "free", drop = TRUE) +
  coord_flip() -> plot_tweets_per_party_word_count_stemmed

plot_tweets_per_party_word_count_stemmed

ggsave(plot_tweets_per_party_word_count_stemmed,
       file = "img/plot_tweets_per_party_word_count_stemmed.pdf")
```


## Stemm party variants (spdde --> spd)

Also, there are duplicates such as "spdbt" "spdde" and "spd"; those instances should be stemmed to "spd", and the like.

```{r}

stem_term <- function(df, col, stem_output){
 # this function looks for '\\w+stem_output\\w+' and replaces each element by `stem_output`
  
    # parameters: 
  # df: data, 
  # col: column to be stemmed, 
  # stem_output: desired output after stemming
  
  # output
  # vector of tokens, where hits are replaced
  
  
  col_quo <- enquo(col)
  col_name <- quo_name(col_quo)
  stem_quo <- enquo(stem_output)
  stem_name <- quo_name(stem_quo)
  
  
  # gsub/str_replace do not understands functions with more than 1 parameter
  pattern1 <- paste0(stem_output,"\\w+")
  pattern2 <- paste0("\\w+",stem_output)
  
    
    df %>% 
      mutate(col_stemmed = str_replace(string = !!col_quo,
                                       pattern = pattern1,
                                       replacement = 
                                       stem_output)) %>% 
      # mutate(col_stemmed = str_replace(string = !!col_quo,
      #                                  pattern = pattern2,
      #                                  replacement = 
      #                                  stem_output)) %>% 
    pull(col_stemmed) -> output
  
  
  return(output)
 
}
```

For comparison, execute this function non-programmatically, but interactively (for debugging reasons):

```{r}
party <- "afd"

my_pattern <- paste0(party, "\\w+")

tweets_per_party_word_count_stemmed %>% 
  mutate(output = str_replace(string = .$token,
                         pattern = my_pattern,
                         replacement = "afd")) %>% 
  filter(str_detect(token, "afd"))



```


Now see whether the function runs as expected:


```{r}
#debug(stem_party)
dummy <- stem_term(df = tweets_per_party_word_count_stemmed,
                     col = token,
                     stem_output = "afd")

dummy %>% 
  as_tibble %>% 
  filter(str_detect(dummy, "afd"))
```



Get different parties:

```{r}
parties <- polits_df %>% filter(!is.na(party)) %>% pull(party) %>% unique
parties
```



```{r}
tweets_per_party_word_count_stemmed2 <- tweets_per_party_word_count_stemmed


for(i in seq_along(parties)){
  tweets_per_party_word_count_stemmed2$token <- 
    stem_term(df = tweets_per_party_word_count_stemmed,
              col = token,
              stem_output = parties[i])  
  
}

```

Plot again with party-related terms stemmed:

```{r}
tweets_per_party_word_count_stemmed2 %>% 
  filter(!(party %in% c("NA", "fraktionslos", NA))) %>% 
  #sample_n(1000) %>% 
  group_by(party) %>% 
  arrange(-n) %>% 
  slice(1:10) %>% 
  ungroup %>% 
  mutate(token = factor(token)) %>% 
  ggplot() +
  aes(x = reorder(token, n), y = n) +
  geom_col() +
  facet_wrap(~party, scales = "free", drop = TRUE) +
  labs(x = "Wort",
       y = "Anzahl") +
  coord_flip() -> plot_tweets_per_party_word_count_stemmed_2

plot_tweets_per_party_word_count_stemmed_2

ggsave(plot_tweets_per_party_word_count_stemmed_2,
       file = "plot_tweets_per_party_word_count_stemmed_2.pdf")

```



# Word lengths

## Histogram of word length
```{r plot-word-distrib}
tweet_tokens %>% 
  mutate(word_length = str_length(token)) %>% 
  group_by(screenName) %>% 
  ggplot +
  aes(x = word_length) +
  geom_histogram() +
  xlim(0,30) +
  labs(x = "Wortlänge",
       y = "Häufigkeit") -> word_distrib


word_distrib


ggsave(word_distrib,
      file = "img/word_distrib.pdf") 

```


# Word lengths 


## Word lenghts per person

```{r}
tweet_tokens %>% 
  mutate(word_length = str_length(token)) %>% 
  group_by(screenName) %>% 
  summarise(word_length_md = median(word_length)) -> word_length_screenName
```

Join to polits_df

```{r}
polits_df %>% 
  left_join(word_length_screenName) -> polits_df
```

## Plot word lengths per party

```{r}

polits_df %>% 
  select(word_length_md, party) %>% 
  group_by(party) %>% 
  summarise(word_length_md_party_md = median(word_length_md, na.rm = T)) %>% 
  ggplot +
  aes(y = word_length_md_party_md, x = reorder(party, word_length_md_party_md)) +
  geom_col() +
  labs(x = "Partei",
       y = "Wortlänge (Median)") +
  coord_flip() -> word_length_party


word_length_party

ggsave(word_length_party, file = "img/word_length_party.pdf")


```



## Comp word lengths

```{r comp-word-lengths}
tweet_tokens %>% 
  mutate(word_length = str_length(token)) %>% 
  group_by(screenName) %>% 
  summarise(word_length_avg = mean(word_length, na.rm = T),
            word_length_md = median(word_length, na.rm = T)) %>% 
  ungroup -> screenName_word_lengths


screenName_word_lengths %>% 
  filter(str_detect(screenName, "real"))

```


## Join to polits-df

```{r}
screenName_word_lengths %>% 
  left_join(polits_df) -> polits_df
```



# What's speaks the AfD about what the others are not interested in?

## Top 100 token per party
```{r}

parties_df <- data_frame(
  party = parties,
  party_ID = 1:length(parties)
)
tweets_per_party_word_count_stemmed %>% 
  mutate(party = str_to_lower(party)) %>% 
  group_by(party) %>% 
  arrange(-n) %>% 
  slice(1:100) %>% 
  left_join(parties_df, by = "party") %>% 
  filter(!is.na(party_ID)) -> top_100s

top_100s %>% 
  count(party_ID)
```


This one is still buggy:


```{r eval = FALSE}
mat <- matrix(0, nrow = 9, ncol = 9)

setdiff(top_100s$token[top_100s$party_ID == 8],
        top_100s$token[top_100s$party_ID == 9])


outer(1:nrow(mat), 1:ncol(mat), FUN = function(i,j) setdiff(top_100s$token[top_100s$party_ID == i],top_100s$token[top_100s$party_ID == j]))



```




# Save token data



```{r}

save(tweet_tokens, file = "../data_polit_twitter/tweet_tokens.Rdata")

save(tweets_token_including_stopwords, file = "../data_polit_twitter/tweets_token_including_stopwords.Rdata")

save(screenName_word_lengths,
     file = "../data_polit_twitter/screenName_word_lengths.Rdata")


save(polits_df, 
     file = "../data_polit_twitter/polits_df.Rdata")

```

